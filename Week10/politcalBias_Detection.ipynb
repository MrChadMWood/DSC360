{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936a3258",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "Title: 10.3 Project Milestone 4\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "Date: 17 Jan 2022\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "Description: This program is project milestone 4 towards the term project. It demonstrates setting up a deep neural network for the purpose of detecting bias in news articles.\n",
    "\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae40da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362042dd",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c2b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [r'data\\articles1.csv', \n",
    "         r'data\\articles2.csv', \n",
    "         r'data\\articles3.csv']\n",
    "\n",
    "# Loads all articles\n",
    "articles = pd.concat((pd.read_csv(f, usecols=['publication', 'content']) for f in files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports allsides.com dataset for determining bias\n",
    "bias_df = pd.read_csv('https://raw.githubusercontent.com/favstats/AllSideR/master/data/allsides_data.csv',\n",
    "                      usecols=['news_source', 'rating'])\n",
    "\n",
    "# Creates list of unique publishers\n",
    "publishers = articles.publication.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7638f",
   "metadata": {},
   "source": [
    "#### Bias-Scoring The Data\n",
    "Building the score dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern identifying publishers\n",
    "publisher = '|'.join(r'(?:{})'.format(x) for x in publishers)\n",
    "\n",
    "# Selects publishers from \n",
    "df = bias_df.loc[bias_df['news_source'].str.contains(publisher, case=False)]\n",
    "\n",
    "# Replaces bias_df publisher names with articles publisher names\n",
    "pub_scores = df.copy()\n",
    "for pub in publishers:\n",
    "    pub_scores.loc[pub_scores.news_source.str.contains(pub, case=False), 'news_source'] = pub\n",
    "\n",
    "# Defines 3 positions for bias and scores them\n",
    "label = '(left|center|right)'\n",
    "scores = {'left': 0, 'center': 1, 'right':2}\n",
    "\n",
    "# Creates score column with score for each publishers rating\n",
    "pub_scores['score'] = pub_scores['rating'].str.extract(label)[0].map(scores)\n",
    "\n",
    "# Drops duplicate rows and redundant columns\n",
    "pub_scores.drop_duplicates(['news_source'], inplace=True)\n",
    "pub_scores.drop(columns=['rating'], inplace=True)\n",
    "\n",
    "# Converts to dictionary\n",
    "pub_scores = dict(zip(pub_scores.news_source, pub_scores.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254126a",
   "metadata": {},
   "source": [
    "Applying the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1da5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes articles that were not scored\n",
    "articles = articles.loc[articles['publication'].isin(pub_scores.keys())].copy()\n",
    "\n",
    "# Added scores as column for each publication\n",
    "articles['scores'] = articles['publication'].apply(lambda x: pub_scores.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31bc7c",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63718d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Loads spacy and customized stop_words \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('against')\n",
    "\n",
    "def normalize(corpus):   \n",
    "    # General Cleansing\n",
    "    html = re.compile('<.*?>') # Compiles regex for HTML tags\n",
    "    corpus = corpus.apply(lambda x: re.sub(html, '', x)) # Removes HTML tags\n",
    "    corpus = corpus.apply(lambda x: re.sub(r'\\S*https?:\\S*', '', x)) # Removes links\n",
    "    corpus = corpus.apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\", '', x)) # Removes mentions\n",
    "    corpus = corpus.apply(lambda x: re.sub('#([a-zA-Z0-9_]{1,50})', '', x)) # Removes hashtags    \n",
    "    corpus = corpus.apply(lambda x: re.sub(r'[^a-zA-z\\s]', '', str(x))) # Removes special characters\n",
    "    corpus = corpus.apply(lambda x: re.sub(' +', ' ', x)) # Removes double+ spaces\n",
    "    corpus = corpus.apply(lambda x: x.strip()) # Removes extra whitspaces\n",
    "\n",
    "    # Runs text through pipeline\n",
    "    clean_list = [] # Preserves cleaned text\n",
    "    tok_list = [] # Preserves tokens\n",
    "    for doc in nlp.pipe(corpus):\n",
    "        tokens = doc\n",
    "        clean_text = (' '.join(word.lemma_ # Returns roots...\n",
    "                               if word.lemma_ != '-PRON-' # ...Unless POS is pronoun...\n",
    "                               else word.text for word in doc # ...Then returns text for pronouns\n",
    "                               if word.lemma_ not in stop_words)) # Filters stopwords\n",
    "        \n",
    "        tok_list.append(tokens) # Returns tokens\n",
    "        clean_list.append(clean_text) # Returns clean text\n",
    "        \n",
    "    # Clean text to lowercase as Series\n",
    "    clean_Series = pd.Series(clean_list).apply(lambda x: str(x).lower()) \n",
    "    \n",
    "    return clean_Series, pd.Series(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc580e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans text and retrieves spacy tokens\n",
    "articles['norm_content'], articles['tokens'] = normalize(articles['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f171792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects an even portion of each bias, then shuffles randomly\n",
    "# Sample size of 1000 per group to reduce computation time\n",
    "data = articles.dropna().groupby(['scores']).sample(n=1000, random_state=1).sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e581ee",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643fdb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: Train  (36550,) Validate  (7832,) Test (7833,)\n",
      "Sentiments: Train  (36550,) Validate  (7832,) Test (7833,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Splits data for supervised model\n",
    "train, validate, test = np.split(data.sample(frac=1, random_state=42), # Shuffles data\n",
    "                                 [int(.7*len(data)), int(.85*len(data))]) # Splits 0-70%; 70-85%; 85-100%\n",
    "\n",
    "\n",
    "print('Reviews:',\n",
    "      'Train ', train.norm_content.shape, \n",
    "      'Validate ', validate.norm_content.shape, \n",
    "      'Test', test.norm_content.shape)\n",
    "\n",
    "print('Sentiments:',\n",
    "      'Train ', train.norm_content.shape, \n",
    "      'Validate ', validate.norm_content.shape, \n",
    "      'Test', test.norm_content.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0366ee",
   "metadata": {},
   "source": [
    "#### Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b9b006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange # For status bar\n",
    "\n",
    "# Usiversal Sentence Encoder Setup\n",
    "flow_graph = tf.Graph()\n",
    "with flow_graph.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\") \n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "flow_graph.finalize()\n",
    "\n",
    "# Initializing TensorFlow\n",
    "session = tf.Session(graph = flow_graph)\n",
    "session.run(init_op)\n",
    "\n",
    "# Computes embeddings in sentences\n",
    "def sim_matrix(merge_list):\n",
    "    # Array for embeddings (512 features per text)\n",
    "    all_embeddings = np.zeros([len(merge_list),512])\n",
    "    \n",
    "    # Builds matrix of all embeddings\n",
    "    for i in trange(0,len(merge_list)):\n",
    "        i_embedding = session.run(embedded_text, feed_dict={text_input: [merge_list[i]]})\n",
    "        all_embeddings[i,:] = i_embedding\n",
    "        \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "298d5f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3000/3000 [1:00:54<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "features_array = sim_matrix(data.norm_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484daa",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates sentence embeding feature\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='content',\n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\", # Leverages Universal Sentence Encoder\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c30531ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Optimizer:\n",
    "optimize = tf.keras.optimizers.Adam(lr=0.00015) \n",
    "\n",
    "# Creates NN with two hidden layers of 40 neurons\n",
    "DNN = Sequential()\n",
    "DNN.add(Dense(40, input_dim=512, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "DNN.add(Dropout(0.25))\n",
    "DNN.add(Dense(40, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "DNN.add(Dropout(0.25))\n",
    "\n",
    "# Output layer of NN\n",
    "DNN.add(Dense(4,activation='softmax'))\n",
    "\n",
    "# Compiles the model\n",
    "DNN.compile(loss='sparse_categorical_crossentropy', optimizer=optimize, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b0891",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d20fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e52048db",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f03ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
