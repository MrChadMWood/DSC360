{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936a3258",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "Title: 12.3 Term Project\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "Date: 5 Mar 2022\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "Description: This program is project milestone 4 towards the term project. It demonstrates building and evaluating a deep neural network to predict bias in news articles. The models training data was severly limited due to computational power, and accuracy reflects that. It will be interesting to increase the models training data on a more powerful computer.\n",
    "\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362042dd",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c2b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [r'data\\articles1.csv', \n",
    "         r'data\\articles2.csv', \n",
    "         r'data\\articles3.csv']\n",
    "\n",
    "# Loads all articles\n",
    "articles = pd.concat((pd.read_csv(f, usecols=['publication', 'content']) for f in files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports allsides.com dataset for determining bias\n",
    "bias_df = pd.read_csv('https://raw.githubusercontent.com/favstats/AllSideR/master/data/allsides_data.csv',\n",
    "                      usecols=['news_source', 'rating'])\n",
    "\n",
    "# Creates list of unique publishers\n",
    "publishers = articles.publication.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7638f",
   "metadata": {},
   "source": [
    "#### Bias-Scoring The Data\n",
    "Building the score dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern identifying publishers\n",
    "publisher = '|'.join(r'(?:{})'.format(x) for x in publishers)\n",
    "\n",
    "# Selects publishers from \n",
    "df = bias_df.loc[bias_df['news_source'].str.contains(publisher, case=False)]\n",
    "\n",
    "# Replaces bias_df publisher names with articles publisher names\n",
    "pub_scores = df.copy()\n",
    "for pub in publishers:\n",
    "    pub_scores.loc[pub_scores.news_source.str.contains(pub, case=False), 'news_source'] = pub\n",
    "\n",
    "# Defines 3 positions for bias and scores them\n",
    "label = '(left|center|right)'\n",
    "scores = {'left': 0, 'center': 1, 'right':2}\n",
    "\n",
    "# Creates score column with score for each publishers rating\n",
    "pub_scores['score'] = pub_scores['rating'].str.extract(label)[0].map(scores)\n",
    "\n",
    "# Drops duplicate rows and redundant columns\n",
    "pub_scores.drop_duplicates(['news_source'], inplace=True)\n",
    "pub_scores.drop(columns=['rating'], inplace=True)\n",
    "\n",
    "# Converts to dictionary\n",
    "pub_scores = dict(zip(pub_scores.news_source, pub_scores.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254126a",
   "metadata": {},
   "source": [
    "Applying the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1da5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes articles that were not scored\n",
    "articles = articles.loc[articles['publication'].isin(pub_scores.keys())].copy()\n",
    "\n",
    "# Added scores as column for each publication\n",
    "articles['scores'] = articles['publication'].apply(lambda x: pub_scores.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31bc7c",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63718d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Loads spacy and customized stop_words \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('against')\n",
    "\n",
    "def normalize(corpus):   \n",
    "    # General Cleansing\n",
    "    html = re.compile('<.*?>') # Compiles regex for HTML tags\n",
    "    corpus = corpus.apply(lambda x: re.sub(html, '', x)) # Removes HTML tags\n",
    "    corpus = corpus.apply(lambda x: re.sub(r'\\S*https?:\\S*', '', x)) # Removes links\n",
    "    corpus = corpus.apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\", '', x)) # Removes mentions\n",
    "    corpus = corpus.apply(lambda x: re.sub('#([a-zA-Z0-9_]{1,50})', '', x)) # Removes hashtags    \n",
    "    corpus = corpus.apply(lambda x: re.sub(r'[^a-zA-z\\s]', '', str(x))) # Removes special characters\n",
    "    corpus = corpus.apply(lambda x: re.sub(' +', ' ', x)) # Removes double+ spaces\n",
    "    corpus = corpus.apply(lambda x: x.strip()) # Removes extra whitspaces\n",
    "\n",
    "    # Runs text through pipeline\n",
    "    clean_list = [] # Preserves cleaned text\n",
    "    tok_list = [] # Preserves tokens\n",
    "    for doc in nlp.pipe(corpus):\n",
    "        tokens = doc\n",
    "        clean_text = (' '.join(word.lemma_ # Returns roots...\n",
    "                               if word.lemma_ != '-PRON-' # ...Unless POS is pronoun...\n",
    "                               else word.text for word in doc # ...Then returns text for pronouns\n",
    "                               if word.lemma_ not in stop_words)) # Filters stopwords\n",
    "        \n",
    "        tok_list.append(tokens) # Returns tokens\n",
    "        clean_list.append(clean_text) # Returns clean text\n",
    "        \n",
    "    # Clean text to lowercase as Series\n",
    "    clean_Series = pd.Series(clean_list).apply(lambda x: str(x).lower()) \n",
    "    \n",
    "    return clean_Series, pd.Series(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc580e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans text and retrieves spacy tokens\n",
    "articles['norm_content'], articles['tokens'] = normalize(articles['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a91727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects an even portion of each bias, then shuffles randomly\n",
    "# Sample size of 1000 per group to reduce computation time\n",
    "data = articles.dropna().groupby(['scores']).sample(n=1000, random_state=1).sample(frac=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e164cf",
   "metadata": {},
   "source": [
    "#### Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import trange # For status bar\n",
    "\n",
    "# Usiversal Sentence Encoder Setup\n",
    "flow_graph = tf.Graph()\n",
    "with flow_graph.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/2\") \n",
    "    embedded_text = embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "flow_graph.finalize()\n",
    "\n",
    "# Initializing TensorFlow\n",
    "session = tf.Session(graph = flow_graph)\n",
    "session.run(init_op)\n",
    "\n",
    "# Computes embeddings in sentences\n",
    "def sim_matrix(merge_list):\n",
    "    # Array for embeddings (512 features per text)\n",
    "    all_embeddings = np.zeros([len(merge_list),512])\n",
    "    \n",
    "    # Builds matrix of all embeddings\n",
    "    for i in trange(0,len(merge_list)):\n",
    "        i_embedding = session.run(embedded_text, feed_dict={text_input: [merge_list[i]]})\n",
    "        all_embeddings[i,:] = i_embedding\n",
    "        \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71b2c98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3000/3000 [1:00:54<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "features_array = sim_matrix(data.norm_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d6558",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee5b159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Optimizer:\n",
    "optimize = tf.keras.optimizers.Adam(learning_rate=0.0005) \n",
    "\n",
    "# Creates NN with two hidden layers of 40 neurons\n",
    "DNN = Sequential()\n",
    "DNN.add(Dense(40, input_dim=512, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "DNN.add(Dropout(0.25))\n",
    "DNN.add(Dense(40, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "DNN.add(Dropout(0.25))\n",
    "\n",
    "# Output layer of NN\n",
    "DNN.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compiles the model\n",
    "DNN.compile(loss='sparse_categorical_crossentropy', optimizer=optimize, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973fd91",
   "metadata": {},
   "source": [
    "#### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2644d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Train 2400, Validate 300, Test 300\n",
      "y: Train 2400, Validate 300, Test 300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Splits data for train aand test\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20)\n",
    "for train_index, test_index in sss.split(features_array, data.scores.values):\n",
    "    \n",
    "    # Splits features for train, test\n",
    "    X_train, X_test = scaler.fit_transform(features_array)[train_index], \\\n",
    "    scaler.fit_transform(features_array)[test_index]\n",
    "    \n",
    "    # Splits bias scores for train, test\n",
    "    y_train, y_test = data.scores.values[train_index], data.scores.values[test_index]\n",
    "    \n",
    "    \n",
    "# Splits test for validate and test\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.50)\n",
    "for train_index, test_index in sss.split(X_test, y_test):\n",
    "    \n",
    "    # Splits features for train, test\n",
    "    X_validate, X_test = scaler.fit_transform(X_test)[train_index], \\\n",
    "    scaler.fit_transform(X_test)[test_index]\n",
    "    \n",
    "    # Splits bias scores for train, test\n",
    "    y_validate, y_test = y_test[train_index], y_test[test_index]\n",
    "    \n",
    "print(f'X: Train {len(X_train)}, Validate {len(X_validate)}, Test {len(X_test)}')\n",
    "print(f'y: Train {len(y_train)}, Validate {len(y_validate)}, Test {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420f5ed",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d2029a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = DNN.fit(X_train, \n",
    "                  y_train,\n",
    "                  batch_size=30,\n",
    "                  epochs=300, \n",
    "                  validation_data=(X_validate, y_validate),                  \n",
    "                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f01c83",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "88a4c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step - loss: 1.1640 - acc: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.163988709449768, 0.44999998807907104]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN.evaluate(X_test, y_test, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
