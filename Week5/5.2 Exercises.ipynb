{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaa1370",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "\n",
    "Title: 5.2 Exercises\n",
    "\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "\n",
    "Date: 24 Jan 2022\n",
    "\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "\n",
    "Description: This program demonstrates cleaning, normalizing, and comparing text data with various NLP techniques.\n",
    "\n",
    "\n",
    "=========================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24b6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalizer as norm\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('data/twitter_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446265f",
   "metadata": {},
   "source": [
    "### 1. Using the file, twitter_sample.csv file, which can be found in the \"data\" directory in the Week 5 GitHub repository: Clean the “Tweet Content” column by removing non-text data and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba928bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Content</th>\n",
       "      <th>Tweet Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pets change lives become part families Thats m...</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another spot morethanmedicine bus bristol week...</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great team HealthSourceOH Local morethanmedici...</td>\n",
       "      <td>ReTweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great team HealthSourceOH Local morethanmedici...</td>\n",
       "      <td>ReTweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great team HealthSourceOH Local morethanmedici...</td>\n",
       "      <td>ReTweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Tweet Content Tweet Type\n",
       "0  Pets change lives become part families Thats m...      Tweet\n",
       "1  Another spot morethanmedicine bus bristol week...      Tweet\n",
       "2  great team HealthSourceOH Local morethanmedici...    ReTweet\n",
       "3  great team HealthSourceOH Local morethanmedici...    ReTweet\n",
       "4  great team HealthSourceOH Local morethanmedici...    ReTweet"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_content = norm.Normalizer(tweets['Tweet Content'])\n",
    "tweets['Tweet Content'] = tweets_content.normalize(strip_html=True, remove_special_chars=True, \n",
    "                                                   remove_digits=True, remove_stopwords=True)\n",
    "\n",
    "tweets[['Tweet Content', 'Tweet Type']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75aa0a",
   "metadata": {},
   "source": [
    "### 2. Filtering only tweets (not re-tweets) use your class from part one of this exercise to build BOW and TF-IDF Vectorizer representations of the text; print your results. Don't over-think this, leverage what the author does in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85597867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>academic</th>\n",
       "      <th>acep</th>\n",
       "      <th>act</th>\n",
       "      <th>activity</th>\n",
       "      <th>advice</th>\n",
       "      <th>advocating</th>\n",
       "      <th>affected</th>\n",
       "      <th>aflac</th>\n",
       "      <th>ag_em</th>\n",
       "      <th>againthe</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoonosen</th>\n",
       "      <th>zoonoses</th>\n",
       "      <th>zu</th>\n",
       "      <th>zwierzt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 933 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   academic  acep  act  activity  advice  advocating  affected  aflac  ag_em  \\\n",
       "0         0     0    0         0       0           0         0      0      0   \n",
       "1         0     0    0         0       0           0         0      0      0   \n",
       "2         0     0    0         0       0           0         0      0      0   \n",
       "3         0     0    0         0       0           0         0      0      1   \n",
       "4         0     0    0         0       0           0         0      0      0   \n",
       "\n",
       "   againthe  ...  yesterday  yet  youll  youre  zoetis  zones  zoonosen  \\\n",
       "0         0  ...          0    0      0      0       0      0         0   \n",
       "1         0  ...          0    0      0      0       0      0         0   \n",
       "2         0  ...          0    0      0      0       0      0         0   \n",
       "3         0  ...          0    0      0      0       0      0         0   \n",
       "4         0  ...          0    0      0      0       0      0         0   \n",
       "\n",
       "   zoonoses  zu  zwierzt  \n",
       "0         0   0        0  \n",
       "1         0   0        0  \n",
       "2         0   0        0  \n",
       "3         0   0        0  \n",
       "4         0   0        0  \n",
       "\n",
       "[5 rows x 933 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Filters tweets from retweets\n",
    "tweets_filtered = tweets['Tweet Content'].loc[tweets['Tweet Type'] == 'Tweet']\n",
    "\n",
    "# Gets bag of words features\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_X = cv.fit_transform(tweets_filtered)\n",
    "cv_names = cv.get_feature_names()\n",
    "\n",
    "pd.DataFrame(cv_X.toarray(), columns=cv_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "daa5ade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>academic</th>\n",
       "      <th>acep</th>\n",
       "      <th>act</th>\n",
       "      <th>activity</th>\n",
       "      <th>advice</th>\n",
       "      <th>advocating</th>\n",
       "      <th>affected</th>\n",
       "      <th>aflac</th>\n",
       "      <th>ag_em</th>\n",
       "      <th>againthe</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>zoetis</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoonosen</th>\n",
       "      <th>zoonoses</th>\n",
       "      <th>zu</th>\n",
       "      <th>zwierzt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 933 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   academic  acep  act  activity  advice  advocating  affected  aflac  ag_em  \\\n",
       "0       0.0   0.0  0.0       0.0     0.0         0.0       0.0    0.0   0.00   \n",
       "1       0.0   0.0  0.0       0.0     0.0         0.0       0.0    0.0   0.00   \n",
       "2       0.0   0.0  0.0       0.0     0.0         0.0       0.0    0.0   0.00   \n",
       "3       0.0   0.0  0.0       0.0     0.0         0.0       0.0    0.0   0.26   \n",
       "4       0.0   0.0  0.0       0.0     0.0         0.0       0.0    0.0   0.00   \n",
       "\n",
       "   againthe  ...  yesterday  yet  youll  youre  zoetis  zones  zoonosen  \\\n",
       "0       0.0  ...        0.0  0.0    0.0    0.0     0.0    0.0       0.0   \n",
       "1       0.0  ...        0.0  0.0    0.0    0.0     0.0    0.0       0.0   \n",
       "2       0.0  ...        0.0  0.0    0.0    0.0     0.0    0.0       0.0   \n",
       "3       0.0  ...        0.0  0.0    0.0    0.0     0.0    0.0       0.0   \n",
       "4       0.0  ...        0.0  0.0    0.0    0.0     0.0    0.0       0.0   \n",
       "\n",
       "   zoonoses   zu  zwierzt  \n",
       "0       0.0  0.0      0.0  \n",
       "1       0.0  0.0      0.0  \n",
       "2       0.0  0.0      0.0  \n",
       "3       0.0  0.0      0.0  \n",
       "4       0.0  0.0      0.0  \n",
       "\n",
       "[5 rows x 933 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Initializes transformer instance\n",
    "tfid = TfidfTransformer(norm='l2', use_idf=True)\n",
    "# Fit to data, followed by transform count matrix\n",
    "tfid_X = tfid.fit_transform(cv_X)\n",
    "\n",
    "pd.DataFrame(np.round(tfid_X.toarray(), 2), columns=cv_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec26010",
   "metadata": {},
   "source": [
    "### 3. Find one or more documents (each tweet is a document) that are similar to each other using Cosine Similarity; print your results. (NOTE: the lower the Cosine Similarity, the more likely the documents are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fcd5a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Builds cosine_similarity dataframe\n",
    "similarity_X = cosine_similarity(tfid_X)\n",
    "similarity_df = pd.DataFrame(similarity_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7fdf4eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_x</th>\n",
       "      <th>Doc_y</th>\n",
       "      <th>value</th>\n",
       "      <th>tweet_x</th>\n",
       "      <th>tweet_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>0.671577</td>\n",
       "      <td>world lost incredible educator Mrs Ventura tau...</td>\n",
       "      <td>world lost incredible educator Mrs Ventura tau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>0.671577</td>\n",
       "      <td>world lost incredible educator Mrs Ventura tau...</td>\n",
       "      <td>world lost incredible educator Mrs Ventura tau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>Czy wiesz e bez lekw dla zwierzt potrzeba wice...</td>\n",
       "      <td>study showed without animalmedicines would nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>study showed without animalmedicines would nee...</td>\n",
       "      <td>Czy wiesz e bez lekw dla zwierzt potrzeba wice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>0.532554</td>\n",
       "      <td>delivered th Special Aflac Duck yesterday prou...</td>\n",
       "      <td>Episode Life Residency Handsome Discussion joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>46</td>\n",
       "      <td>0.077824</td>\n",
       "      <td>Impfen schtzt Der beste Weg zu mehr Tiergesund...</td>\n",
       "      <td>Episode Life Residency Handsome Discussion joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.069634</td>\n",
       "      <td>FIX Want preview AG_EM story check back Monday...</td>\n",
       "      <td>Medtronic Engage patients even smile morethanm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>0.064783</td>\n",
       "      <td>Genetic information provided scant incremental...</td>\n",
       "      <td>delivered th Special Aflac Duck yesterday prou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>great team HealthSourceOH Local morethanmedici...</td>\n",
       "      <td>nearly years PennyBrohnUK link standard medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>FIX Want preview AG_EM story check back Monday...</td>\n",
       "      <td>nearly years PennyBrohnUK link standard medica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Doc_x  Doc_y     value                                            tweet_x  \\\n",
       "76     76     77  0.671577  world lost incredible educator Mrs Ventura tau...   \n",
       "77     77     76  0.671577  world lost incredible educator Mrs Ventura tau...   \n",
       "18     18     24  0.609183  Czy wiesz e bez lekw dla zwierzt potrzeba wice...   \n",
       "24     24     18  0.609183  study showed without animalmedicines would nee...   \n",
       "26     26     48  0.532554  delivered th Special Aflac Duck yesterday prou...   \n",
       "..    ...    ...       ...                                                ...   \n",
       "73     73     46  0.077824  Impfen schtzt Der beste Weg zu mehr Tiergesund...   \n",
       "9       9     13  0.069634  FIX Want preview AG_EM story check back Monday...   \n",
       "25     25     26  0.064783  Genetic information provided scant incremental...   \n",
       "4       4     22  0.009435  great team HealthSourceOH Local morethanmedici...   \n",
       "6       6     22  0.008022  FIX Want preview AG_EM story check back Monday...   \n",
       "\n",
       "                                              tweet_y  \n",
       "76  world lost incredible educator Mrs Ventura tau...  \n",
       "77  world lost incredible educator Mrs Ventura tau...  \n",
       "18  study showed without animalmedicines would nee...  \n",
       "24  Czy wiesz e bez lekw dla zwierzt potrzeba wice...  \n",
       "26  Episode Life Residency Handsome Discussion joi...  \n",
       "..                                                ...  \n",
       "73  Episode Life Residency Handsome Discussion joi...  \n",
       "9   Medtronic Engage patients even smile morethanm...  \n",
       "25  delivered th Special Aflac Duck yesterday prou...  \n",
       "4   nearly years PennyBrohnUK link standard medica...  \n",
       "6   nearly years PennyBrohnUK link standard medica...  \n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes instances where index meets column at same document (resulting in ~1)\n",
    "similarity_df[similarity_df > 0.999] = np.NaN\n",
    "\n",
    "# Creates ranked df containing {column: index} for highest value in rows\n",
    "ranked_simularity = similarity_df.idxmax().reset_index()\n",
    "ranked_simularity.columns=['Doc_x', 'Doc_y']\n",
    "\n",
    "# Adds values in as new column for convenience\n",
    "ranked_simularity['value'] = ranked_simularity.apply(lambda x: similarity_df.loc[x['Doc_x'], x['Doc_y']], axis=1)\n",
    "\n",
    "# Adds the matching tweets\n",
    "ranked_simularity['tweet_x'] = ranked_simularity.apply(lambda x: tweets['Tweet Content'].loc[x['Doc_x']], axis=1)\n",
    "ranked_simularity['tweet_y'] = ranked_simularity.apply(lambda x: tweets['Tweet Content'].loc[x['Doc_y']], axis=1)\n",
    "\n",
    "ranked_simularity.sort_values(by=['value'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
