{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8eba55",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "\n",
    "Title: 7.2 Exercises\n",
    "\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "\n",
    "Date: 31 Jan 2022\n",
    "\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "\n",
    "Description: This program demonstrates the use of different methods for key phrase extraction (Lord of The Rings Dialog) and topic modeling (research articles) to build and test a machine learning model that performs topic summarization on text documents. The model uses Latent Semantic Indexing.\n",
    "\n",
    "\n",
    "=========================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e3318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lotr_script = pd.read_csv('data/lotr_scripts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d3e4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char</th>\n",
       "      <th>dialog</th>\n",
       "      <th>movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAGOL</td>\n",
       "      <td>Oh Smeagol Ive got one! , Ive got a fish Smeag...</td>\n",
       "      <td>The Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SMEAGOL</td>\n",
       "      <td>Pull it in! Go on, go on, go on, pull it in!</td>\n",
       "      <td>The Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEAGOL</td>\n",
       "      <td>Arrghh!</td>\n",
       "      <td>The Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMEAGOL</td>\n",
       "      <td>Deagol!</td>\n",
       "      <td>The Return of the King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMEAGOL</td>\n",
       "      <td>Deagol!</td>\n",
       "      <td>The Return of the King</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char                                             dialog  \\\n",
       "0   DEAGOL  Oh Smeagol Ive got one! , Ive got a fish Smeag...   \n",
       "1  SMEAGOL     Pull it in! Go on, go on, go on, pull it in!     \n",
       "2   DEAGOL                                           Arrghh!    \n",
       "3  SMEAGOL                                          Deagol!     \n",
       "4  SMEAGOL                                          Deagol!     \n",
       "\n",
       "                     movie  \n",
       "0  The Return of the King   \n",
       "1  The Return of the King   \n",
       "2  The Return of the King   \n",
       "3  The Return of the King   \n",
       "4  The Return of the King   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_script.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90d1b1",
   "metadata": {},
   "source": [
    "### Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5a0cb",
   "metadata": {},
   "source": [
    "<b><i>*    Collocations</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf41b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh Smeagol Ive got one! , Ive got a fish Smeagol, Smeagol!     \n",
      " oh smeagol ive got one ive got fish smeagol smeagol\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import lib.normalizer as nm\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "\n",
    "norm_lotr = nm.Normalizer(lotr_script.dialog)\n",
    "norm_lotr = norm_lotr.normalize(\n",
    "    strip_html=True, remove_special_chars=True, \n",
    "    remove_digits=True, remove_stopwords=True,\n",
    "    remove_accented_chars=True, expand_contractions=True,\n",
    "    text_lower=True, to_str=True)\n",
    "\n",
    "# Compare first lines\n",
    "print(lotr_script.dialog[0], '\\n', norm_lotr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5bdb407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('can not', 75),\n",
       "  ('mr frodo', 48),\n",
       "  ('let us', 18),\n",
       "  ('minas tirith', 16),\n",
       "  ('middle earth', 12),\n",
       "  ('let go', 11),\n",
       "  ('frodo frodo', 10),\n",
       "  ('must go', 9),\n",
       "  ('helms deep', 9),\n",
       "  ('peregrin took', 9)],\n",
       " [('grond grond grond', 7),\n",
       "  ('can not hold', 5),\n",
       "  ('can not get', 5),\n",
       "  ('death death death', 4),\n",
       "  ('han mathon ne', 4),\n",
       "  ('heh heh heh', 4),\n",
       "  ('hmm hmm hmm', 4),\n",
       "  ('baggins baggins baggins', 4),\n",
       "  ('hold mr frodo', 3),\n",
       "  ('can not leave', 3)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates trailing list sequences\n",
    "def compute_ngrams(sequence, n): # n -> degree of n-gram\n",
    "    return list(\n",
    "        zip(*(sequence[index:]\n",
    "              for index in range(n)))\n",
    "    )\n",
    "\n",
    "# Creates single body of text\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip()\n",
    "                     for document in corpus])\n",
    "\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams) # Records dict of each outcome occurrences\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), # Orders dict ascending\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit] # Trims sorted_ngrams_fd\n",
    "    sorted_ngrams = [(' '.join(text), freq) # Dict to List comp\n",
    "                     for text, freq in sorted_ngrams]\n",
    "    \n",
    "    return sorted_ngrams\n",
    "\n",
    "\n",
    "top_bigrams = get_top_ngrams(corpus=norm_lotr, ngram_val=2, limit=10)\n",
    "top_trigrams = get_top_ngrams(corpus=norm_lotr, ngram_val=3, limit=10)\n",
    "\n",
    "top_bigrams, top_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0fac3",
   "metadata": {},
   "source": [
    "<b><i>* Weighted Tag-Based Phrase Extraction</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4e7a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sentences = nm.Normalizer(lotr_script.dialog)\n",
    "norm_sentences = norm_sentences.normalize(\n",
    "    strip_html=True, remove_special_chars=True, \n",
    "    remove_digits=True, remove_accented_chars=True, \n",
    "    expand_contractions=True, to_str=True)\n",
    "\n",
    "# Tokenizes dialog\n",
    "tok_sentences = lotr_script.dialog.apply(lambda x: nltk.sent_tokenize(str(x)))\n",
    "\n",
    "# Returns sentences longer than 15 words for better data\n",
    "# long_sents = tok_sentences.loc[tok_sentences.apply(lambda x: len(' '.join(x).split()) > 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f2dd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Returns noun phrase chunks from text list\n",
    "def get_chunks(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', \n",
    "               stopword_list=stopwords):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))] # Tags POS in tokenized text\n",
    "\n",
    "        # Chunks text by regex\n",
    "        chunks = [chunker.parse(tagged_sent)\n",
    "                  for tagged_sent in tagged_sents]\n",
    "        # Returns list of (word, tag, IOB-tag)\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]    \n",
    "        # Flattens list\n",
    "        flattened_chunks = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                wtc_sent for wtc_sent in wtc_sents)\n",
    "        )        \n",
    "        # Filters for non-0 chunks\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk])\n",
    "                           for status, chunk\n",
    "                           in itertools.groupby(flattened_chunks,\n",
    "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')] \n",
    "        # Filters stopwords\n",
    "        valid_chunks = [' '.join(word.lower()\n",
    "                                 for word, tag, chunk in wtc_group\n",
    "                                 if word.lower() not in stopword_list)\n",
    "                        for status, wtc_group in valid_chunks_tagged if status]     \n",
    "        \n",
    "        all_chunks.append(valid_chunks)\n",
    "        \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e618cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Tag-Based Phrase Extraction\n",
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}', \n",
    "                                  top_n=10):\n",
    "    # Flattens series to list of str\n",
    "    sentences = [item for sublist in sentences for item in sublist]\n",
    "    # Chunks text to noun phrases\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "    # Maps norm words to integer IDs\n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    # Bag of Words\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    # Uses dict integer IDs for weights\n",
    "    weighted_phrases = {dictionary.get(idx): value\n",
    "                        for doc in corpus_tfidf\n",
    "                        for idx, value in doc}\n",
    "    # Sorts ascending\n",
    "    weighted_phrases = sorted(weighted_phrases.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1181fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smeagol ive', 1.0),\n",
       " ('arrghh', 1.0),\n",
       " ('murderer', 1.0),\n",
       " ('name', 1.0),\n",
       " ('oooohhh', 1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tfidf_weighted_keyphrases(sentences=tok_sentences, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2cfeae",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e16867b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_PATH = 'data/nipstxt/'\n",
    "folders = ['nips{0:02}'.format(i) for i in range(0,13)]\n",
    "\n",
    "articles = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(data_PATH+folder)\n",
    "    for file_name in file_names:\n",
    "        with open(data_PATH + folder+'/'+file_name, encoding='utf-8',\n",
    "                  errors='ignore',mode='r+') as f:\n",
    "            data = f.read()\n",
    "        articles.append(data)\n",
    "\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e66cb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Normalizes data \n",
    "def normalizer_toker(articles):\n",
    "    norm_articles = []\n",
    "    for article in articles:\n",
    "        article = article.lower()\n",
    "        article_tokens = [token.strip() for token in wtk.tokenize(article)]\n",
    "        # Cleans tokens of numerics, i < 1, and stopwords\n",
    "        # Updated from textbook to encode str into byte object\n",
    "        article_tokens = [token for token in article_tokens if \n",
    "                          not token.isnumeric() and len(token) > 1 and token not in stop_words]\n",
    "        # Removes Nonetypes     \n",
    "        article_tokens = list(filter(None, article_tokens))\n",
    "        \n",
    "        if article_tokens:\n",
    "            norm_articles.append(article_tokens)\n",
    "            \n",
    "    return norm_articles    \n",
    "\n",
    "norm_articles = normalizer_toker(articles)\n",
    "len(norm_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f7e6c",
   "metadata": {},
   "source": [
    "<b><i>* Text Representation with Feature Engineering</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b0302247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connectivity', 'versus', 'entropy', 'yaser', 'abu_mostafa', 'california_institute', 'technology_pasadena', 'ca_abstract', 'connectivity', 'neural_network', 'number', 'synapses', 'per', 'neuron', 'relate', 'complexity', 'problems', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean_functions', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gates', 'however', 'network', 'learns', 'problem', 'examples', 'using', 'local', 'learning', 'rule', 'prove', 'entropy', 'problem', 'becomes']\n"
     ]
    }
   ],
   "source": [
    "# Attempts to extract useful bigrams and remove some unuseful bigrams\n",
    "\n",
    "# Textbook uses delimiter=b'_' \n",
    "# ...which no longer works unless using < python3\n",
    "bigram = gensim.models.Phrases(norm_articles, min_count=20, threshold=20, \n",
    "                               delimiter='_')\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Sample\n",
    "print(bigram_model[norm_articles[0]][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a8726ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [(10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated'), (15, 'acknowledgement_work'), (16, 'addison_wesley'), (17, 'afosr'), (18, 'aip'), (19, 'air_force')]\n",
      "Total: 82825\n"
     ]
    }
   ],
   "source": [
    "# Generates phrases for tokenized corpus for phrase : to mapping\n",
    "# Allows ML by providing number tensors\n",
    "\n",
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_articles]\n",
    "\n",
    "# Creates dict of documents\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "\n",
    "print(f'Sample: {list(dictionary.items())[10:20]}')\n",
    "print(f'Total: {len(dictionary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ebd7132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 8684\n"
     ]
    }
   ],
   "source": [
    "# Filters left and right skew outliers\n",
    "# ... by <20 occurences or >60% of docs\n",
    "# This retains common doc-specific words\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print(f'Total: {len(dictionary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6faa449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [(12, 3), (14, 1), (15, 1), (16, 1), (17, 16), (20, 1), (24, 1), (26, 1), (31, 3), (35, 1), (36, 1), (40, 3), (41, 5), (42, 1), (49, 1), (54, 3), (55, 1), (57, 1), (60, 1), (62, 3), (65, 5), (66, 4), (67, 2), (76, 1), (77, 1), (78, 1), (80, 3), (86, 1), (87, 4), (88, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Creates Bag of Words vector\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams] # Returns (tok_id, count)\n",
    "print(f'Sample: {bow_corpus[1][:30]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040ff78",
   "metadata": {},
   "source": [
    "<b><i>* Latent Semantic Indexing</b></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d8c1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi works with assumption that words used in same context have same meaning\n",
    "\n",
    "TOPICS = 10 # Topics used from corpa\n",
    "lsi_bow_model = gensim.models.LsiModel(bow_corpus, id2word=dictionary, # Builds model for LSI\n",
    "                                       num_topics=TOPICS, onepass=True, chunksize=1740, power_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4b324d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.213*\"training\" + 0.147*\"error\" + 0.140*\"state\" + 0.126*\"units\" + 0.125*\"models\" + 0.122*\"weights\" + 0.099*\"parameters\" + 0.098*\"method\" + 0.095*\"unit\" + 0.094*\"neurons\" + 0.093*\"layer\" + 0.092*\"noise\" + 0.090*\"linear\" + 0.089*\"image\" + 0.089*\"vector\" + 0.086*\"patterns\" + 0.084*\"neural_network\" + 0.083*\"functions\" + 0.082*\"control\" + 0.080*\"neuron\"\n",
      "\n",
      "Topic #2:\n",
      "0.297*\"neurons\" + -0.252*\"training\" + 0.248*\"neuron\" + 0.229*\"cells\" + 0.220*\"cell\" + -0.179*\"error\" + 0.157*\"response\" + 0.152*\"activity\" + 0.144*\"visual\" + 0.138*\"stimulus\" + 0.112*\"motion\" + 0.112*\"synaptic\" + 0.104*\"firing\" + -0.100*\"class\" + 0.095*\"neural\" + -0.092*\"method\" + 0.092*\"cortical\" + 0.091*\"circuit\" + 0.090*\"layer\" + 0.086*\"synapses\"\n",
      "\n",
      "Topic #3:\n",
      "-0.488*\"state\" + 0.245*\"training\" + 0.225*\"image\" + -0.195*\"control\" + 0.193*\"units\" + -0.190*\"states\" + 0.147*\"images\" + 0.133*\"layer\" + -0.129*\"action\" + 0.126*\"features\" + -0.125*\"policy\" + 0.114*\"unit\" + -0.113*\"optimal\" + -0.113*\"neuron\" + 0.107*\"recognition\" + -0.101*\"reinforcement_learning\" + 0.092*\"object\" + 0.091*\"trained\" + -0.090*\"actions\" + -0.085*\"neurons\"\n",
      "\n",
      "Topic #4:\n",
      "-0.416*\"image\" + -0.270*\"images\" + 0.228*\"error\" + 0.221*\"weights\" + 0.217*\"training\" + 0.191*\"neuron\" + 0.164*\"weight\" + 0.154*\"neurons\" + -0.144*\"visual\" + -0.142*\"object\" + -0.134*\"models\" + -0.126*\"features\" + 0.125*\"units\" + -0.123*\"state\" + 0.118*\"unit\" + -0.102*\"motion\" + 0.092*\"layer\" + 0.092*\"patterns\" + -0.089*\"objects\" + -0.085*\"local\"\n",
      "\n",
      "Topic #5:\n",
      "0.360*\"state\" + 0.311*\"units\" + 0.196*\"unit\" + 0.190*\"control\" + -0.155*\"distribution\" + 0.141*\"task\" + 0.136*\"training\" + 0.135*\"states\" + -0.117*\"neuron\" + -0.111*\"functions\" + 0.109*\"action\" + -0.109*\"linear\" + -0.106*\"noise\" + -0.105*\"neurons\" + -0.101*\"class\" + 0.101*\"word\" + -0.098*\"gaussian\" + 0.098*\"architecture\" + 0.093*\"net\" + -0.091*\"parameters\"\n",
      "\n",
      "Topic #6:\n",
      "0.404*\"units\" + -0.362*\"training\" + 0.290*\"unit\" + -0.177*\"control\" + 0.159*\"weights\" + -0.157*\"neuron\" + -0.143*\"classifier\" + -0.131*\"neural_network\" + -0.130*\"word\" + -0.124*\"recognition\" + -0.122*\"classifiers\" + -0.119*\"classification\" + -0.111*\"neurons\" + -0.104*\"trained\" + -0.100*\"speech\" + 0.098*\"functions\" + -0.096*\"hmm\" + 0.081*\"approximation\" + 0.080*\"distribution\" + -0.079*\"test\"\n",
      "\n",
      "Topic #7:\n",
      "0.232*\"control\" + 0.230*\"image\" + -0.228*\"models\" + -0.204*\"cells\" + -0.193*\"cell\" + 0.192*\"chip\" + 0.178*\"error\" + 0.159*\"circuit\" + 0.149*\"weight\" + -0.142*\"probability\" + 0.135*\"noise\" + -0.124*\"patterns\" + 0.123*\"analog\" + -0.120*\"hmm\" + -0.118*\"word\" + 0.115*\"images\" + -0.107*\"state\" + 0.105*\"weights\" + -0.103*\"parameters\" + 0.101*\"voltage\"\n",
      "\n",
      "Topic #8:\n",
      "-0.278*\"control\" + -0.259*\"error\" + 0.228*\"neuron\" + -0.212*\"cells\" + -0.168*\"cell\" + -0.156*\"training\" + 0.152*\"neurons\" + -0.146*\"motion\" + 0.131*\"memory\" + 0.121*\"state\" + 0.117*\"node\" + 0.117*\"image\" + 0.117*\"class\" + -0.113*\"visual\" + 0.112*\"chip\" + 0.111*\"recognition\" + -0.110*\"response\" + 0.104*\"analog\" + -0.100*\"controller\" + 0.100*\"circuit\"\n",
      "\n",
      "Topic #9:\n",
      "-0.321*\"noise\" + -0.252*\"models\" + 0.216*\"cell\" + 0.210*\"cells\" + -0.181*\"parameters\" + 0.177*\"class\" + -0.147*\"signal\" + 0.145*\"classifier\" + -0.137*\"units\" + -0.126*\"speech\" + 0.120*\"functions\" + -0.120*\"gaussian\" + -0.113*\"word\" + -0.112*\"hmm\" + 0.110*\"examples\" + 0.108*\"node\" + 0.096*\"algorithms\" + 0.094*\"classification\" + -0.093*\"unit\" + -0.091*\"neuron\"\n",
      "\n",
      "Topic #10:\n",
      "-0.339*\"neurons\" + 0.270*\"circuit\" + -0.235*\"neuron\" + 0.213*\"cell\" + 0.180*\"cells\" + 0.168*\"chip\" + 0.165*\"node\" + 0.151*\"current\" + 0.150*\"voltage\" + -0.148*\"image\" + 0.142*\"models\" + 0.136*\"nodes\" + -0.127*\"patterns\" + 0.126*\"motion\" + 0.124*\"analog\" + -0.123*\"images\" + -0.116*\"noise\" + 0.111*\"word\" + -0.108*\"training\" + -0.098*\"task\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints 10 topics with 20 words from each topic\n",
    "for topic_id, topic in lsi_bow_model.print_topics(num_topics=10, num_words=20):\n",
    "    print(f'Topic #{str(topic_id+1)}:\\n{topic}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bcb3ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "**************************************************\n",
      "Direction 1: [('training', 0.213), ('error', 0.147), ('state', 0.14), ('units', 0.126), ('models', 0.125), ('weights', 0.122), ('parameters', 0.099), ('method', 0.098), ('unit', 0.095), ('neurons', 0.094), ('layer', 0.093), ('noise', 0.092), ('linear', 0.09), ('image', 0.089), ('vector', 0.089), ('patterns', 0.086), ('neural_network', 0.084), ('functions', 0.083), ('control', 0.082), ('neuron', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "Topic #2:\n",
      "**************************************************\n",
      "Direction 1: [('neurons', 0.297), ('neuron', 0.248), ('cells', 0.229), ('cell', 0.22), ('response', 0.157), ('activity', 0.152), ('visual', 0.144), ('stimulus', 0.138), ('motion', 0.112), ('synaptic', 0.112), ('firing', 0.104), ('neural', 0.095), ('cortical', 0.092), ('circuit', 0.091), ('layer', 0.09), ('synapses', 0.086)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.252), ('error', -0.179), ('class', -0.1), ('method', -0.092)]\n",
      "--------------------------------------------------\n",
      "Topic #3:\n",
      "**************************************************\n",
      "Direction 1: [('training', 0.245), ('image', 0.225), ('units', 0.193), ('images', 0.147), ('layer', 0.133), ('features', 0.126), ('unit', 0.114), ('recognition', 0.107), ('object', 0.092), ('trained', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.488), ('control', -0.195), ('states', -0.19), ('action', -0.129), ('policy', -0.125), ('optimal', -0.113), ('neuron', -0.113), ('reinforcement_learning', -0.101), ('actions', -0.09), ('neurons', -0.085)]\n",
      "--------------------------------------------------\n",
      "Topic #4:\n",
      "**************************************************\n",
      "Direction 1: [('error', 0.228), ('weights', 0.221), ('training', 0.217), ('neuron', 0.191), ('weight', 0.164), ('neurons', 0.154), ('units', 0.125), ('unit', 0.118), ('layer', 0.092), ('patterns', 0.092)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.416), ('images', -0.27), ('visual', -0.144), ('object', -0.142), ('models', -0.134), ('features', -0.126), ('state', -0.123), ('motion', -0.102), ('objects', -0.089), ('local', -0.085)]\n",
      "--------------------------------------------------\n",
      "Topic #5:\n",
      "**************************************************\n",
      "Direction 1: [('state', 0.36), ('units', 0.311), ('unit', 0.196), ('control', 0.19), ('task', 0.141), ('training', 0.136), ('states', 0.135), ('action', 0.109), ('word', 0.101), ('architecture', 0.098), ('net', 0.093)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('distribution', -0.155), ('neuron', -0.117), ('functions', -0.111), ('linear', -0.109), ('noise', -0.106), ('neurons', -0.105), ('class', -0.101), ('gaussian', -0.098), ('parameters', -0.091)]\n",
      "--------------------------------------------------\n",
      "Topic #6:\n",
      "**************************************************\n",
      "Direction 1: [('units', 0.404), ('unit', 0.29), ('weights', 0.159), ('functions', 0.098), ('approximation', 0.081), ('distribution', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.362), ('control', -0.177), ('neuron', -0.157), ('classifier', -0.143), ('neural_network', -0.131), ('word', -0.13), ('recognition', -0.124), ('classifiers', -0.122), ('classification', -0.119), ('neurons', -0.111), ('trained', -0.104), ('speech', -0.1), ('hmm', -0.096), ('test', -0.079)]\n",
      "--------------------------------------------------\n",
      "Topic #7:\n",
      "**************************************************\n",
      "Direction 1: [('control', 0.232), ('image', 0.23), ('chip', 0.192), ('error', 0.178), ('circuit', 0.159), ('weight', 0.149), ('noise', 0.135), ('analog', 0.123), ('images', 0.115), ('weights', 0.105), ('voltage', 0.101)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('models', -0.228), ('cells', -0.204), ('cell', -0.193), ('probability', -0.142), ('patterns', -0.124), ('hmm', -0.12), ('word', -0.118), ('state', -0.107), ('parameters', -0.103)]\n",
      "--------------------------------------------------\n",
      "Topic #8:\n",
      "**************************************************\n",
      "Direction 1: [('neuron', 0.228), ('neurons', 0.152), ('memory', 0.131), ('state', 0.121), ('node', 0.117), ('image', 0.117), ('class', 0.117), ('chip', 0.112), ('recognition', 0.111), ('analog', 0.104), ('circuit', 0.1)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('control', -0.278), ('error', -0.259), ('cells', -0.212), ('cell', -0.168), ('training', -0.156), ('motion', -0.146), ('visual', -0.113), ('response', -0.11), ('controller', -0.1)]\n",
      "--------------------------------------------------\n",
      "Topic #9:\n",
      "**************************************************\n",
      "Direction 1: [('cell', 0.216), ('cells', 0.21), ('class', 0.177), ('classifier', 0.145), ('functions', 0.12), ('examples', 0.11), ('node', 0.108), ('algorithms', 0.096), ('classification', 0.094)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('noise', -0.321), ('models', -0.252), ('parameters', -0.181), ('signal', -0.147), ('units', -0.137), ('speech', -0.126), ('gaussian', -0.12), ('word', -0.113), ('hmm', -0.112), ('unit', -0.093), ('neuron', -0.091)]\n",
      "--------------------------------------------------\n",
      "Topic #10:\n",
      "**************************************************\n",
      "Direction 1: [('circuit', 0.27), ('cell', 0.213), ('cells', 0.18), ('chip', 0.168), ('node', 0.165), ('current', 0.151), ('voltage', 0.15), ('models', 0.142), ('nodes', 0.136), ('motion', 0.126), ('analog', 0.124), ('word', 0.111)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neurons', -0.339), ('neuron', -0.235), ('image', -0.148), ('patterns', -0.127), ('images', -0.123), ('noise', -0.116), ('training', -0.108), ('task', -0.098)]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Attempts to separate subthemes within articles by positive vs negative orientation of vector space\n",
    "# As direction\n",
    "for n in range(TOPICS):\n",
    "    print(f'Topic #{str(n+1)}:')\n",
    "    print('*'*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, weight in lsi_bow_model.show_topic(n, topn=20): #topn = number of words to be included\n",
    "        if weight >= 0: # Positively orientated VS\n",
    "            d1.append((term, round(weight, 3)))\n",
    "        else: # Negatively orientated VS\n",
    "            d2.append((term, round(weight, 3)))\n",
    "    print(f'Direction 1: {d1}')\n",
    "    print('-'*50)\n",
    "    print(f'Direction 2: {d2}')\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0393bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8684, 10), (10,), (10, 1740))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempts to get matrices (U, S, VT) from model using SVD\n",
    "\n",
    "term_topic = lsi_bow_model.projection.u\n",
    "singular_values = lsi_bow_model.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow_model[bow_corpus],\n",
    "                 len(singular_values)).T / singular_values).T\n",
    "\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0762f223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
       "0  0.015  0.012 -0.014  0.014 -0.016 -0.004  0.002  0.026  0.007 -0.021\n",
       "1  0.038  0.032 -0.017  0.045 -0.001  0.006  0.059  0.040 -0.048 -0.006\n",
       "2  0.025 -0.001 -0.021  0.021 -0.001  0.011  0.019  0.011  0.000 -0.018\n",
       "3  0.027  0.026 -0.006  0.027 -0.009 -0.015 -0.004  0.042 -0.010 -0.031\n",
       "4  0.036  0.002 -0.018  0.022 -0.021  0.042  0.027  0.050  0.036  0.042"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieves proportion of topic in each doc, showing significance of topic\n",
    "\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3),\n",
    "                              columns=['T'+str(i) for i in range(1, TOPICS+1)])\n",
    "document_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bd42697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc # 13:\n",
      "Top 3 Topics: ['T8', 'T3', 'T9']\n",
      "Article Summary: \n",
      "137 \n",
      "On the \n",
      "Power of Neural Networks for \n",
      "Solving Hard Problems \n",
      "Jehoshua Bruck \n",
      "Joseph W. Goodman \n",
      "Information Systems Laboratory \n",
      "Department of Electrical Engineering \n",
      "Stanford University \n",
      "Stanford, CA 94305 \n",
      "Abstract \n",
      "This paper deals with a neural network model in which each neuron \n",
      "performs a threshold logic function. An important property of the model \n",
      "is that it always converges to a stable state when operating in a serial \n",
      "mode [2,5]. This property is the basis of the potential applicat\n",
      "-------------------------------------------------- \n",
      "\n",
      "Doc # 250:\n",
      "Top 3 Topics: ['T10', 'T1', 'T6']\n",
      "Article Summary: \n",
      "542 Kassebaum, Tenorio and Schaefers \n",
      "The Cocktail Party Problem: \n",
      "Speech/Data Signal Separation Comparison \n",
      "between Backpropagation and SONN \n",
      "John Kassebaum \n",
      "jakec.ecn.purdue.edu \n",
      "Manoel Fernando Tenorio \n",
      "tenorioee.ecn.purdue.edu \n",
      "Chrlstoph Schaefers \n",
      "Parallel Distributed Structures Laboratory \n",
      "School of Electrical Engineering \n",
      "Purdue University \n",
      "W. Lafayette, IN. 47907 \n",
      "ABSTRACT \n",
      "This work introduces a new method called Self Organizing Neural \n",
      "Network (SONN) algorithm and compares its perfor\n",
      "-------------------------------------------------- \n",
      "\n",
      "Doc # 500:\n",
      "Top 3 Topics: ['T7', 'T1', 'T9']\n",
      "Article Summary: \n",
      "Learning Global Direct Inverse Kinematics \n",
      "David DeMers* \n",
      "Computer Science & Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0114 \n",
      "Kenneth Kreutz-Deigado I \n",
      "Electrical & Computer Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0407 \n",
      "Abstract \n",
      "We introduce and demonstrate a bootstrap method for construction of an in- \n",
      "verse function for the robot kinematic mapping using only sample configuration- \n",
      "space/workspace data. Unsupervised learning (clustering) techniques are used on \n",
      "pre-image neighborhoods in order to l\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignores +/- orientation, visually allows comparison between start\n",
    "# ... of doc and topics to verify accuracy\n",
    "\n",
    "doc_numbers = [13, 250, 500]\n",
    "\n",
    "for doc_number in doc_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-\n",
    "                                                        np.absolute(\n",
    "                                                        document_topics.iloc[\n",
    "                                                            doc_number].values))[:3]])\n",
    "    print(f'Doc # {str(doc_number)}:')\n",
    "    print(f'Top 3 Topics: {top_topics}')\n",
    "    print(f'Article Summary: \\n{articles[doc_number][:500]}')\n",
    "    print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54e271",
   "metadata": {},
   "source": [
    "<i>As we can see, document 13 speaks about neurons and neural networks, and the models summarized it into topics ['T10', 'T1', 'T6']; respectively, the cell above which seperates each subtopic by vector spaces yields that out topic summarization model is visually effective. The same stands for our other two tests, doc# 250 and 500.</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
