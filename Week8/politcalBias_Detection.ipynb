{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936a3258",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "Title: 8.2 Project Milestone 3\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "Date: 17 Jan 2022\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "Description: This program is project milestone 3 towards the term project. It demonstrates wrangling the data to be used and feature engineering on its unstructured data (news articles).\n",
    "\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Not all of these are used. As this is a milestone,\n",
    "consider this a draft of the models that may be used.\n",
    "Some modules are still imported at specific cells because\n",
    "it was necessary to restart and import only necessary modules to prevent \n",
    "memory errors from arrising.\n",
    "'''\n",
    "\n",
    "# Typical modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Google SP (linux can use tf_sentencepiece)\n",
    "import sentencepiece\n",
    "\n",
    "# Keras and tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras import backend\n",
    "\n",
    "#sklearn and imblearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362042dd",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [r'data\\articles1.csv', \n",
    "         r'data\\articles2.csv', \n",
    "         r'data\\articles3.csv']\n",
    "\n",
    "# Imports all articles\n",
    "articles = pd.concat((pd.read_csv(f, usecols=['publication', 'content']) for f in files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255e27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_csv('articles_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports allsides.com dataset for determining bias\n",
    "bias_df = pd.read_csv('https://raw.githubusercontent.com/favstats/AllSideR/master/data/allsides_data.csv',\n",
    "                      usecols=['news_source', 'rating'])\n",
    "\n",
    "# Creates list of unique publishers\n",
    "publishers = articles.publication.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7638f",
   "metadata": {},
   "source": [
    "### Building a Bias-Score Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c7ffe",
   "metadata": {},
   "source": [
    "Building the score dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern identifying publishers\n",
    "publisher = '|'.join(r'(?:{})'.format(x) for x in publishers)\n",
    "\n",
    "# Selects publishers from articles articles\n",
    "df = bias_df.loc[bias_df['news_source'].str.contains(publisher, case=False)]\n",
    "\n",
    "# Replaces bias_df publisher names with articles publisher names\n",
    "pub_scores = df.copy()\n",
    "for pub in publishers:\n",
    "    pub_scores.loc[pub_scores.news_source.str.contains(pub, case=False), 'news_source'] = pub\n",
    "\n",
    "# Defines 3 positions for bias and scores them\n",
    "label = '(left|center|right)'\n",
    "scores = {'left': 0, 'center': 1, 'right':2}\n",
    "\n",
    "# Creates score column with score for each publishers rating\n",
    "pub_scores['score'] = pub_scores['rating'].str.extract(label)[0].map(scores)\n",
    "\n",
    "# Drops duplicate rows and redundant columns\n",
    "pub_scores.drop_duplicates(['news_source'], inplace=True)\n",
    "pub_scores.drop(columns=['rating'], inplace=True)\n",
    "\n",
    "# Converts to dictionary\n",
    "pub_scores = dict(zip(pub_scores.news_source, pub_scores.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254126a",
   "metadata": {},
   "source": [
    "Applying the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1da5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes articles that were not scored\n",
    "articles = articles.loc[articles['publication'].isin(pub_scores.keys())].copy()\n",
    "\n",
    "# Added scores as column for each publication\n",
    "articles['scores'] = articles['publication'].apply(lambda x: pub_scores.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31bc7c",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefd7f6",
   "metadata": {},
   "source": [
    "Converts to lower and removes special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ce5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern\n",
    "pattern = r'[^a-zA-z\\s]'\n",
    "\n",
    "\n",
    "articles['norm_content'] = articles.content.copy()\n",
    "# Text to lowercase\n",
    "articles['norm_content'] = articles.norm_content.apply(lambda x: str(x).lower())\n",
    "# Removes special characters\n",
    "articles['norm_content'] = articles.norm_content.apply(lambda x: re.sub(pattern, '', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee1eba",
   "metadata": {},
   "source": [
    "Filters stopwords and lemmatizes text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63718d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat']) \n",
    "\n",
    "# Runs text through pipeline\n",
    "nlp_list = []\n",
    "def nlp_pipe(corpus):\n",
    "    for doc in nlp.pipe(corpus, disable=['parser', 'ner', 'textcat']):\n",
    "        nlp_list.append(' '.join(word.lemma_\n",
    "                                 if word.lemma_ != '-PRON-' # Preserves pronouns\n",
    "                                 else word.text for word in doc # Returns roots\n",
    "                                 if not word.is_stop)) # Filters stopwords\n",
    "\n",
    "    return pd.Series(nlp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc580e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stopwords and lemmatizes text\n",
    "articles['norm_content'] = nlp_pipe(articles['norm_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e581ee",
   "metadata": {},
   "source": [
    "### Collecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643fdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reducing computation time\n",
    "# Collects an even portion of each bias and drops raw content/pub cols\n",
    "data = articles.dropna().groupby(['scores']).sample(n=17405, random_state=1).drop(columns=['content', 'publication'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100ec99",
   "metadata": {},
   "source": [
    "BOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Gets bag of words features\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_X = cv.fit_transform(data.norm_content)\n",
    "cv_names = cv.get_feature_names_out()\n",
    "\n",
    "bow = pd.DataFrame(cv_X.toarray(), columns=cv_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb4ae6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_suwilm</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaajiao</th>\n",
       "      <th>aaany</th>\n",
       "      <th>aaarena</th>\n",
       "      <th>aaas</th>\n",
       "      <th>...</th>\n",
       "      <th>zyngas</th>\n",
       "      <th>zyuganov</th>\n",
       "      <th>zyvex</th>\n",
       "      <th>zywicki</th>\n",
       "      <th>zyzo</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzz</th>\n",
       "      <th>zzzanthropology</th>\n",
       "      <th>zzzs</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132633 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a_suwilm  aa  aaa  aaaaaah  aaaaah  aaaand  aaajiao  aaany  aaarena  aaas  \\\n",
       "0         0   0    0        0       0       0        0      0        0     0   \n",
       "1         0   0    0        0       0       0        0      0        0     0   \n",
       "2         0   0    0        0       0       0        0      0        0     0   \n",
       "3         0   0    0        0       0       0        0      0        0     0   \n",
       "4         0   0    0        0       0       0        0      0        0     0   \n",
       "\n",
       "   ...  zyngas  zyuganov  zyvex  zywicki  zyzo  zz  zzz  zzzanthropology  \\\n",
       "0  ...       0         0      0        0     0   0    0                0   \n",
       "1  ...       0         0      0        0     0   0    0                0   \n",
       "2  ...       0         0      0        0     0   0    0                0   \n",
       "3  ...       0         0      0        0     0   0    0                0   \n",
       "4  ...       0         0      0        0     0   0    0                0   \n",
       "\n",
       "   zzzs  zzzz  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     0  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "\n",
       "[5 rows x 132633 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7992855",
   "metadata": {},
   "source": [
    "TF-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82c93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Initializes transformer instance\n",
    "tfid = TfidfTransformer(norm='l2', use_idf=True)\n",
    "# Fit to data, followed by transform count matrix\n",
    "tfid_X = tfid.fit_transform(cv_X)\n",
    "\n",
    "pd.DataFrame(np.round(tfid_X.toarray(), 2), columns=cv_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
