{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be1d0de",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "Title: 9.2 Exercises\n",
    "\n",
    "Author: Chad Wood\n",
    "\n",
    "Date: 20 Jan 2022\n",
    "\n",
    "Modified By: Chad Wood\n",
    "\n",
    "Description: This program demostrates building a Named Entity Recognition tagger, to include building a NER model, and furthermore contrasting the results with SpaCy's NER engine.\n",
    "\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716eff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   Sentence #  1048575 non-null  object\n",
      " 1   Word        1048575 non-null  object\n",
      " 2   POS         1048575 non-null  object\n",
      " 3   Tag         1048575 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 32.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'data/ner_dataset.csv.gz', compression='gzip',\n",
    "                 encoding='ISO-8859-1')\n",
    "df = df.fillna(method='ffill') # Curious about ffills application in this context.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1deb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sent, i):\n",
    "    \n",
    "    # Current word features\n",
    "    word = sent[i][0] # Instantiates word\n",
    "    postag = sent[i][1] # Instantiates POS tag\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(), # Returns lowercase word\n",
    "        'word[-3:]': word[-3:], # Last 3 chars\n",
    "        'word[-2:]': word[-2:], # Last 2 chars\n",
    "        'word.isupper()': word.isupper(), # Boolean if word uppercase \n",
    "        'word.istitle()': word.istitle(), # Boolean if word title\n",
    "        'word.isdigit()': word.isdigit(), # Boolean if word is digit\n",
    "        'postag': postag, # POS tag\n",
    "        'postag[:2]': postag[:2] # First 2 chars of POS tag\n",
    "    }\n",
    "    \n",
    "    # Previous word features\n",
    "    if i > 0:\n",
    "            word1 = sent[i-1][0] # Instantiates prior word\n",
    "            postag1 = sent[i-1][1] # Instantiates POS tag\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(), # Returns lowercase word\n",
    "                '-1:word.istitle()': word1.istitle(), # Boolean if word title\n",
    "                '-1:word.isupper()': word1.isupper(), # Boolean if word uppercase \n",
    "                '-1:postag': postag1, # POS tag\n",
    "                '-1:postag[:2]': postag1[:2] # First 2 chars of POS tag\n",
    "            })\n",
    "    else:\n",
    "        features['BOS'] = True # 'Beginning of Sentence'\n",
    "    \n",
    "    # Next word features  \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0] # Instantiates next word\n",
    "        postag1 = sent[i+1][1] # Instantiates POS tag\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(), # Returns lowercase word\n",
    "            '+1:word.istitle()': word1.istitle(), # Boolean if word title\n",
    "            '+1:word.isupper()': word1.isupper(), # Boolean if word uppercase \n",
    "            '+1:postag': postag1, # POS tag\n",
    "            '+1:postag[:2]': postag1[:2] # First 2 chars of POS tag\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # 'End of Sentence'\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b9e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates list of word features for each word in sentence\n",
    "def sent_features(sent):\n",
    "    return [word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "# Generates list of NER labels for each sentence\n",
    "def sent_labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195ea2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates list of word, POS, and NER tag for each word in each sentence by ziping columns values\n",
    "agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                   s['POS'].values.tolist(),\n",
    "                                                   s['Tag'].values.tolist())]\n",
    "\n",
    "# Groups data and applies prior function to wrangle data into desired format\n",
    "grouped_df = df.groupby('Sentence #').apply(agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67901222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 'NNS', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('demonstrators', 'NNS', 'O'),\n",
       " ('have', 'VBP', 'O'),\n",
       " ('marched', 'VBN', 'O')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts grouped_df to list of nested lists for each sentence, nested tupple for each (w, p, t) \n",
    "sentences = [s for s in grouped_df] # Essentially pd.Series.tolist()\n",
    "sentences[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b63cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'marched',\n",
       "  'word[-3:]': 'hed',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBN',\n",
       "  'postag[:2]': 'VB',\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'through',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'through',\n",
       "  'word[-3:]': 'ugh',\n",
       "  'word[-2:]': 'gh',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'marched',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:postag': 'VBN',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  'EOS': True}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_features(sentences[0][4:6]) # Demonstrates features of words 4-5 in sentence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e75db6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33571,), (14388,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Stores list data in memory as an array for creating model\n",
    "X = np.array([sent_features(s) for s in sentences], dtype=object)\n",
    "y = np.array([sent_labels(s) for s in sentences], dtype=object)\n",
    "\n",
    "# Splits data for observed and predict values for training/testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape # Outputs size to verify split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e08910cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|███████████████████████████████████████| 33571/33571 [00:12<00:00, 2709.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 129580\n",
      "Seconds required: 2.663\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=3.92  loss=1180124.79 active=128640 feature_norm=1.00\n",
      "Iter 2   time=3.95  loss=927293.60 active=127333 feature_norm=4.42\n",
      "Iter 3   time=2.03  loss=724497.81 active=122222 feature_norm=3.87\n",
      "Iter 4   time=9.01  loss=393567.11 active=123154 feature_norm=3.24\n",
      "Iter 5   time=1.72  loss=330653.15 active=125018 feature_norm=4.04\n",
      "Iter 6   time=1.91  loss=242580.46 active=118725 feature_norm=6.18\n",
      "Iter 7   time=1.78  loss=205070.81 active=111791 feature_norm=8.00\n",
      "Iter 8   time=1.79  loss=183148.35 active=107309 feature_norm=8.86\n",
      "Iter 9   time=1.81  loss=166599.67 active=102936 feature_norm=10.21\n",
      "Iter 10  time=1.79  loss=149841.00 active=99329 feature_norm=11.77\n",
      "Iter 11  time=1.81  loss=138759.61 active=97933 feature_norm=13.07\n",
      "Iter 12  time=1.87  loss=131646.27 active=97020 feature_norm=14.01\n",
      "Iter 13  time=1.88  loss=120360.54 active=93382 feature_norm=16.56\n",
      "Iter 14  time=1.85  loss=111667.37 active=91337 feature_norm=18.25\n",
      "Iter 15  time=1.84  loss=103424.23 active=90058 feature_norm=19.80\n",
      "Iter 16  time=1.95  loss=98903.17 active=88757 feature_norm=21.65\n",
      "Iter 17  time=1.87  loss=93620.60 active=87303 feature_norm=23.64\n",
      "Iter 18  time=1.82  loss=89008.61 active=86298 feature_norm=25.92\n",
      "Iter 19  time=1.73  loss=84471.29 active=85431 feature_norm=28.49\n",
      "Iter 20  time=1.68  loss=79825.37 active=84392 feature_norm=32.02\n",
      "Iter 21  time=1.67  loss=75701.02 active=84275 feature_norm=34.93\n",
      "Iter 22  time=1.77  loss=71546.61 active=83135 feature_norm=39.22\n",
      "Iter 23  time=1.84  loss=68054.16 active=81789 feature_norm=42.76\n",
      "Iter 24  time=1.91  loss=64835.10 active=80377 feature_norm=48.14\n",
      "Iter 25  time=1.97  loss=62034.67 active=79965 feature_norm=51.38\n",
      "Iter 26  time=1.89  loss=59438.42 active=79197 feature_norm=55.76\n",
      "Iter 27  time=1.89  loss=56606.47 active=78569 feature_norm=61.15\n",
      "Iter 28  time=1.70  loss=54120.52 active=77518 feature_norm=66.44\n",
      "Iter 29  time=1.84  loss=51900.46 active=76954 feature_norm=71.19\n",
      "Iter 30  time=1.99  loss=49839.21 active=75907 feature_norm=78.09\n",
      "Iter 31  time=1.84  loss=48131.69 active=75758 feature_norm=84.14\n",
      "Iter 32  time=1.79  loss=46419.23 active=74787 feature_norm=89.09\n",
      "Iter 33  time=1.90  loss=44961.87 active=74669 feature_norm=94.98\n",
      "Iter 34  time=1.97  loss=43239.02 active=73762 feature_norm=104.03\n",
      "Iter 35  time=1.89  loss=41759.78 active=72829 feature_norm=112.34\n",
      "Iter 36  time=1.82  loss=40510.97 active=72655 feature_norm=119.39\n",
      "Iter 37  time=1.85  loss=38842.92 active=71475 feature_norm=131.72\n",
      "Iter 38  time=1.79  loss=38446.48 active=70711 feature_norm=142.87\n",
      "Iter 39  time=1.74  loss=36965.47 active=71094 feature_norm=147.73\n",
      "Iter 40  time=2.00  loss=36311.27 active=71050 feature_norm=152.36\n",
      "Iter 41  time=2.00  loss=35453.80 active=70292 feature_norm=162.66\n",
      "Iter 42  time=1.64  loss=34644.70 active=70311 feature_norm=169.27\n",
      "Iter 43  time=1.72  loss=34151.02 active=70170 feature_norm=174.03\n",
      "Iter 44  time=1.69  loss=33591.20 active=69697 feature_norm=179.83\n",
      "Iter 45  time=1.64  loss=33352.25 active=69119 feature_norm=186.39\n",
      "Iter 46  time=1.74  loss=32954.79 active=69007 feature_norm=187.75\n",
      "Iter 47  time=1.54  loss=32751.21 active=68566 feature_norm=189.75\n",
      "Iter 48  time=1.65  loss=32474.82 active=66994 feature_norm=193.46\n",
      "Iter 49  time=1.67  loss=32357.86 active=65388 feature_norm=196.63\n",
      "Iter 50  time=1.64  loss=32154.80 active=65093 feature_norm=197.71\n",
      "Iter 51  time=1.70  loss=32057.62 active=64289 feature_norm=198.69\n",
      "Iter 52  time=1.58  loss=31895.80 active=63601 feature_norm=200.23\n",
      "Iter 53  time=1.66  loss=31762.70 active=62582 feature_norm=200.79\n",
      "Iter 54  time=1.55  loss=31631.33 active=62403 feature_norm=201.66\n",
      "Iter 55  time=1.67  loss=31519.03 active=61918 feature_norm=202.59\n",
      "Iter 56  time=1.72  loss=31440.70 active=60724 feature_norm=204.48\n",
      "Iter 57  time=1.58  loss=31317.77 active=61020 feature_norm=204.83\n",
      "Iter 58  time=1.68  loss=31269.12 active=60893 feature_norm=205.23\n",
      "Iter 59  time=1.73  loss=31159.44 active=60273 feature_norm=206.64\n",
      "Iter 60  time=1.67  loss=31118.74 active=60154 feature_norm=207.29\n",
      "Iter 61  time=1.72  loss=31032.18 active=60285 feature_norm=207.78\n",
      "Iter 62  time=1.80  loss=30985.00 active=60085 feature_norm=208.19\n",
      "Iter 63  time=1.78  loss=30919.41 active=59751 feature_norm=208.80\n",
      "Iter 64  time=1.80  loss=30865.26 active=59534 feature_norm=209.35\n",
      "Iter 65  time=1.86  loss=30814.18 active=59523 feature_norm=209.64\n",
      "Iter 66  time=2.03  loss=30767.53 active=59323 feature_norm=209.92\n",
      "Iter 67  time=1.76  loss=30720.19 active=59089 feature_norm=210.18\n",
      "Iter 68  time=1.83  loss=30680.52 active=58938 feature_norm=210.42\n",
      "Iter 69  time=1.71  loss=30644.21 active=58782 feature_norm=210.56\n",
      "Iter 70  time=1.62  loss=30607.16 active=58587 feature_norm=210.75\n",
      "Iter 71  time=1.68  loss=30570.97 active=58416 feature_norm=210.88\n",
      "Iter 72  time=1.61  loss=30540.48 active=58278 feature_norm=211.06\n",
      "Iter 73  time=1.72  loss=30511.16 active=58183 feature_norm=211.18\n",
      "Iter 74  time=1.60  loss=30483.68 active=57966 feature_norm=211.31\n",
      "Iter 75  time=1.67  loss=30456.30 active=57763 feature_norm=211.40\n",
      "Iter 76  time=1.67  loss=30430.74 active=57601 feature_norm=211.56\n",
      "Iter 77  time=1.73  loss=30406.60 active=57479 feature_norm=211.64\n",
      "Iter 78  time=1.68  loss=30384.12 active=57315 feature_norm=211.80\n",
      "Iter 79  time=1.68  loss=30362.78 active=57131 feature_norm=211.88\n",
      "Iter 80  time=1.69  loss=30343.65 active=57007 feature_norm=212.01\n",
      "Iter 81  time=1.75  loss=30325.61 active=56957 feature_norm=212.06\n",
      "Iter 82  time=1.71  loss=30307.32 active=56874 feature_norm=212.17\n",
      "Iter 83  time=1.61  loss=30292.15 active=56826 feature_norm=212.22\n",
      "Iter 84  time=1.66  loss=30275.51 active=56712 feature_norm=212.34\n",
      "Iter 85  time=1.50  loss=30262.03 active=56616 feature_norm=212.39\n",
      "Iter 86  time=1.52  loss=30246.36 active=56527 feature_norm=212.53\n",
      "Iter 87  time=1.55  loss=30233.03 active=56487 feature_norm=212.58\n",
      "Iter 88  time=1.67  loss=30219.73 active=56415 feature_norm=212.68\n",
      "Iter 89  time=1.70  loss=30209.49 active=56269 feature_norm=212.73\n",
      "Iter 90  time=1.65  loss=30197.91 active=56155 feature_norm=212.84\n",
      "Iter 91  time=1.70  loss=30187.18 active=56082 feature_norm=212.89\n",
      "Iter 92  time=1.67  loss=30176.00 active=56045 feature_norm=212.99\n",
      "Iter 93  time=1.68  loss=30166.81 active=56032 feature_norm=213.03\n",
      "Iter 94  time=1.68  loss=30156.86 active=55957 feature_norm=213.14\n",
      "Iter 95  time=1.51  loss=30148.68 active=55917 feature_norm=213.18\n",
      "Iter 96  time=1.50  loss=30138.45 active=55871 feature_norm=213.30\n",
      "Iter 97  time=1.69  loss=30131.37 active=55802 feature_norm=213.35\n",
      "Iter 98  time=1.68  loss=30122.84 active=55786 feature_norm=213.47\n",
      "Iter 99  time=1.52  loss=30115.52 active=55756 feature_norm=213.52\n",
      "Iter 100 time=1.67  loss=30107.55 active=55730 feature_norm=213.62\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 185.748\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 55730 (129580)\n",
      "Number of active attributes: 28179 (87806)\n",
      "Number of active labels: 17 (17)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "# Configurations\n",
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', # Training alg. https://en.wikipedia.org/wiki/Limited-memory_BFGS\n",
    "                           c1=0.1, # Coefficient for Lasso\n",
    "                           c2=0.1, # Coefficient for Ridge\n",
    "                           max_iterations=100,\n",
    "                           all_possible_transitions=True,\n",
    "                           verbose=True)\n",
    "\n",
    "# Passes on issue: https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60\n",
    "# Alt solution is downgrading package \n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aab0f578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8542474536519857"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics as crf_metrics\n",
    "\n",
    "# Lists all labels, removes 0 (outside NER)\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "crf_metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d215994",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following produces a TypeError that I can't seem to find a solution for, \n",
    "aside from perhaps downgrading my module installation (untested). I don't see many\n",
    "duplicate issues online, and I'm considering submit this to the appropriate GitHub \n",
    "as an issue.\n",
    "\n",
    "Issue seems to be with module recognizing three positional arguments given. This \n",
    "still occurs even when arguments are reduced to only 'y_test, y_pred'. I attempted \n",
    "working around this by using sklearn's metrics.classification_report, only to find \n",
    "that the datatype used is no longer accepted due to a depreciation.\n",
    "'''\n",
    "\n",
    "crf_metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73e2d1",
   "metadata": {},
   "source": [
    "1. Run the following sentence through your tagger: “Fourteen days ago, Emperor Palpatine left San Diego, CA for Tatooine to follow Luke Skywalker.” Report on the tags applied to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b389a152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fourteen</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>days</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emperor</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Palpatine</td>\n",
       "      <td>I-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>left</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Diego</td>\n",
       "      <td>I-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CA</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tatooine</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>follow</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Luke</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Skywalker</td>\n",
       "      <td>I-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text    NER\n",
       "0    Fourteen  B-per\n",
       "1        days      O\n",
       "2         ago      O\n",
       "3           ,      O\n",
       "4     Emperor  B-per\n",
       "5   Palpatine  I-per\n",
       "6        left      O\n",
       "7         San  B-geo\n",
       "8       Diego  I-geo\n",
       "9           ,      O\n",
       "10         CA  B-org\n",
       "11        for  I-org\n",
       "12   Tatooine  I-org\n",
       "13         to      O\n",
       "14     follow      O\n",
       "15       Luke  B-per\n",
       "16  Skywalker  I-per\n",
       "17          .      O"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = 'Fourteen days ago, Emperor Palpatine left San Diego, CA for Tatooine to follow Luke Skywalker.'\n",
    "\n",
    "# Retrieves text POS\n",
    "text_tokens = nltk.word_tokenize(text)\n",
    "text_pos = nltk.pos_tag(text_tokens)\n",
    "\n",
    "# Retrieves features\n",
    "features = [sent_features(text_pos)]\n",
    "\n",
    "# Generates labels\n",
    "labels = crf.predict(features)\n",
    "text_labels = labels[0]\n",
    "\n",
    "# Formats report\n",
    "text_ner_df = pd.DataFrame([[token, tag] for token, tag in zip(text_tokens, text_labels)], columns=['Text', 'NER'])\n",
    "text_ner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132322ba",
   "metadata": {},
   "source": [
    "2. Run the same sentence through spaCy’s NER engine.\n",
    "3. Compare and contrast the results – you can do this in your Jupyter Notebook or as a comment in your .py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ecd4fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>SpaCy</th>\n",
       "      <th>Homebrew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fourteen</td>\n",
       "      <td>DATE</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>days</td>\n",
       "      <td>DATE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago</td>\n",
       "      <td>DATE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emperor</td>\n",
       "      <td></td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Palpatine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>I-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>left</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Diego</td>\n",
       "      <td>GPE</td>\n",
       "      <td>I-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CA</td>\n",
       "      <td></td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tatooine</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>follow</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Luke</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Skywalker</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text    SpaCy Homebrew\n",
       "0    Fourteen     DATE    B-per\n",
       "1        days     DATE        O\n",
       "2         ago     DATE        O\n",
       "3           ,                 O\n",
       "4     Emperor             B-per\n",
       "5   Palpatine      GPE    I-per\n",
       "6        left                 O\n",
       "7         San      GPE    B-geo\n",
       "8       Diego      GPE    I-geo\n",
       "9           ,                 O\n",
       "10         CA             B-org\n",
       "11        for             I-org\n",
       "12   Tatooine  PRODUCT    I-org\n",
       "13         to                 O\n",
       "14     follow                 O\n",
       "15       Luke   PERSON    B-per\n",
       "16  Skywalker   PERSON    I-per\n",
       "17          .                 O"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp_text = nlp(text)\n",
    "\n",
    "spacy_ner = pd.DataFrame([(word.text, word.ent_type_) for word in nlp_text], columns=['Text', 'SpaCy'])\n",
    "spacy_ner['Homebrew'] = text_ner_df['NER']\n",
    "\n",
    "spacy_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa649d75",
   "metadata": {},
   "source": [
    "SpaCy's NER engine seems better suited for the text data provided, mostly because it's able to accurately recognize more entities than the NER engine built here. However, the version built here could likely be optimized to perform better by tuning the model and perhaps adjusting the data used for training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
